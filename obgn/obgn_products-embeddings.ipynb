{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hindu-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node2vec\n",
    "\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn import Node2Vec\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beautiful-particular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = f'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "southern-absence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 123718280], x=[2449029, 100], y=[2449029, 1])\n"
     ]
    }
   ],
   "source": [
    "dataset = PygNodePropPredDataset(name = \"ogbn-products\", root = 'dataset/')\n",
    "data = dataset[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "racial-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embedding(model):\n",
    "    torch.save(model.embedding.weight.data.cpu(), 'embedding.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fixed-listing",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "walk_length = 40\n",
    "context_size = 20\n",
    "walks_per_node = 10\n",
    "batch_size = 256\n",
    "lr = 0.01\n",
    "epochs = 1\n",
    "log_steps = 100\n",
    "\n",
    "model = Node2Vec(data.edge_index, embedding_dim, walk_length,\n",
    "                     context_size, walks_per_node,\n",
    "                     sparse=True).to(device)\n",
    "\n",
    "loader = model.loader(batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=4)\n",
    "    \n",
    "optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "built-paintball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Step: 100/9567, Loss: 9.4703\n",
      "Epoch: 01, Step: 200/9567, Loss: 8.5829\n",
      "Epoch: 01, Step: 300/9567, Loss: 7.7455\n",
      "Epoch: 01, Step: 400/9567, Loss: 6.9069\n",
      "Epoch: 01, Step: 500/9567, Loss: 6.1520\n",
      "Epoch: 01, Step: 600/9567, Loss: 5.4254\n",
      "Epoch: 01, Step: 700/9567, Loss: 4.7844\n",
      "Epoch: 01, Step: 800/9567, Loss: 4.2113\n",
      "Epoch: 01, Step: 900/9567, Loss: 3.7529\n",
      "Epoch: 01, Step: 1000/9567, Loss: 3.3591\n",
      "Epoch: 01, Step: 1100/9567, Loss: 3.0074\n",
      "Epoch: 01, Step: 1200/9567, Loss: 2.7340\n",
      "Epoch: 01, Step: 1300/9567, Loss: 2.4927\n",
      "Epoch: 01, Step: 1400/9567, Loss: 2.3274\n",
      "Epoch: 01, Step: 1500/9567, Loss: 2.1452\n",
      "Epoch: 01, Step: 1600/9567, Loss: 1.9810\n",
      "Epoch: 01, Step: 1700/9567, Loss: 1.8911\n",
      "Epoch: 01, Step: 1800/9567, Loss: 1.7995\n",
      "Epoch: 01, Step: 1900/9567, Loss: 1.7030\n",
      "Epoch: 01, Step: 2000/9567, Loss: 1.6219\n",
      "Epoch: 01, Step: 2100/9567, Loss: 1.5631\n",
      "Epoch: 01, Step: 2200/9567, Loss: 1.4980\n",
      "Epoch: 01, Step: 2300/9567, Loss: 1.4357\n",
      "Epoch: 01, Step: 2400/9567, Loss: 1.3865\n",
      "Epoch: 01, Step: 2500/9567, Loss: 1.3491\n",
      "Epoch: 01, Step: 2600/9567, Loss: 1.2983\n",
      "Epoch: 01, Step: 2700/9567, Loss: 1.2671\n",
      "Epoch: 01, Step: 2800/9567, Loss: 1.2258\n",
      "Epoch: 01, Step: 2900/9567, Loss: 1.2014\n",
      "Epoch: 01, Step: 3000/9567, Loss: 1.1803\n",
      "Epoch: 01, Step: 3100/9567, Loss: 1.1566\n",
      "Epoch: 01, Step: 3200/9567, Loss: 1.1363\n",
      "Epoch: 01, Step: 3300/9567, Loss: 1.1131\n",
      "Epoch: 01, Step: 3400/9567, Loss: 1.0965\n",
      "Epoch: 01, Step: 3500/9567, Loss: 1.0820\n",
      "Epoch: 01, Step: 3600/9567, Loss: 1.0638\n",
      "Epoch: 01, Step: 3700/9567, Loss: 1.0457\n",
      "Epoch: 01, Step: 3800/9567, Loss: 1.0418\n",
      "Epoch: 01, Step: 3900/9567, Loss: 1.0283\n",
      "Epoch: 01, Step: 4000/9567, Loss: 1.0183\n",
      "Epoch: 01, Step: 4100/9567, Loss: 1.0093\n",
      "Epoch: 01, Step: 4200/9567, Loss: 0.9969\n",
      "Epoch: 01, Step: 4300/9567, Loss: 0.9876\n",
      "Epoch: 01, Step: 4400/9567, Loss: 0.9825\n",
      "Epoch: 01, Step: 4500/9567, Loss: 0.9731\n",
      "Epoch: 01, Step: 4600/9567, Loss: 0.9688\n",
      "Epoch: 01, Step: 4700/9567, Loss: 0.9602\n",
      "Epoch: 01, Step: 4800/9567, Loss: 0.9574\n",
      "Epoch: 01, Step: 4900/9567, Loss: 0.9518\n",
      "Epoch: 01, Step: 5000/9567, Loss: 0.9468\n",
      "Epoch: 01, Step: 5100/9567, Loss: 0.9399\n",
      "Epoch: 01, Step: 5200/9567, Loss: 0.9355\n",
      "Epoch: 01, Step: 5300/9567, Loss: 0.9355\n",
      "Epoch: 01, Step: 5400/9567, Loss: 0.9342\n",
      "Epoch: 01, Step: 5500/9567, Loss: 0.9299\n",
      "Epoch: 01, Step: 5600/9567, Loss: 0.9221\n",
      "Epoch: 01, Step: 5700/9567, Loss: 0.9188\n",
      "Epoch: 01, Step: 5800/9567, Loss: 0.9192\n",
      "Epoch: 01, Step: 5900/9567, Loss: 0.9086\n",
      "Epoch: 01, Step: 6000/9567, Loss: 0.9118\n",
      "Epoch: 01, Step: 6100/9567, Loss: 0.9105\n",
      "Epoch: 01, Step: 6200/9567, Loss: 0.9091\n",
      "Epoch: 01, Step: 6300/9567, Loss: 0.9023\n",
      "Epoch: 01, Step: 6400/9567, Loss: 0.9045\n",
      "Epoch: 01, Step: 6500/9567, Loss: 0.8985\n",
      "Epoch: 01, Step: 6600/9567, Loss: 0.9037\n",
      "Epoch: 01, Step: 6700/9567, Loss: 0.9025\n",
      "Epoch: 01, Step: 6800/9567, Loss: 0.9015\n",
      "Epoch: 01, Step: 6900/9567, Loss: 0.8977\n",
      "Epoch: 01, Step: 7000/9567, Loss: 0.8927\n",
      "Epoch: 01, Step: 7100/9567, Loss: 0.8935\n",
      "Epoch: 01, Step: 7200/9567, Loss: 0.8949\n",
      "Epoch: 01, Step: 7300/9567, Loss: 0.8904\n",
      "Epoch: 01, Step: 7400/9567, Loss: 0.8873\n",
      "Epoch: 01, Step: 7500/9567, Loss: 0.8871\n",
      "Epoch: 01, Step: 7600/9567, Loss: 0.8894\n",
      "Epoch: 01, Step: 7700/9567, Loss: 0.8841\n",
      "Epoch: 01, Step: 7800/9567, Loss: 0.8895\n",
      "Epoch: 01, Step: 7900/9567, Loss: 0.8788\n",
      "Epoch: 01, Step: 8000/9567, Loss: 0.8866\n",
      "Epoch: 01, Step: 8100/9567, Loss: 0.8829\n",
      "Epoch: 01, Step: 8200/9567, Loss: 0.8787\n",
      "Epoch: 01, Step: 8300/9567, Loss: 0.8784\n",
      "Epoch: 01, Step: 8400/9567, Loss: 0.8776\n",
      "Epoch: 01, Step: 8500/9567, Loss: 0.8823\n",
      "Epoch: 01, Step: 8600/9567, Loss: 0.8772\n",
      "Epoch: 01, Step: 8700/9567, Loss: 0.8807\n",
      "Epoch: 01, Step: 8800/9567, Loss: 0.8802\n",
      "Epoch: 01, Step: 8900/9567, Loss: 0.8718\n",
      "Epoch: 01, Step: 9000/9567, Loss: 0.8739\n",
      "Epoch: 01, Step: 9100/9567, Loss: 0.8740\n",
      "Epoch: 01, Step: 9200/9567, Loss: 0.8721\n",
      "Epoch: 01, Step: 9300/9567, Loss: 0.8734\n",
      "Epoch: 01, Step: 9400/9567, Loss: 0.8754\n",
      "Epoch: 01, Step: 9500/9567, Loss: 0.8699\n",
      "Epoch: 02, Step: 100/9567, Loss: 0.8671\n",
      "Epoch: 02, Step: 200/9567, Loss: 0.8782\n",
      "Epoch: 02, Step: 300/9567, Loss: 0.8659\n",
      "Epoch: 02, Step: 400/9567, Loss: 0.8731\n",
      "Epoch: 02, Step: 500/9567, Loss: 0.8697\n",
      "Epoch: 02, Step: 600/9567, Loss: 0.8661\n",
      "Epoch: 02, Step: 700/9567, Loss: 0.8626\n",
      "Epoch: 02, Step: 800/9567, Loss: 0.8656\n",
      "Epoch: 02, Step: 900/9567, Loss: 0.8691\n",
      "Epoch: 02, Step: 1000/9567, Loss: 0.8678\n",
      "Epoch: 02, Step: 1100/9567, Loss: 0.8706\n",
      "Epoch: 02, Step: 1200/9567, Loss: 0.8690\n",
      "Epoch: 02, Step: 1300/9567, Loss: 0.8639\n",
      "Epoch: 02, Step: 1400/9567, Loss: 0.8660\n",
      "Epoch: 02, Step: 1500/9567, Loss: 0.8652\n",
      "Epoch: 02, Step: 1600/9567, Loss: 0.8687\n",
      "Epoch: 02, Step: 1700/9567, Loss: 0.8608\n",
      "Epoch: 02, Step: 1800/9567, Loss: 0.8717\n",
      "Epoch: 02, Step: 1900/9567, Loss: 0.8635\n",
      "Epoch: 02, Step: 2000/9567, Loss: 0.8629\n",
      "Epoch: 02, Step: 2100/9567, Loss: 0.8639\n",
      "Epoch: 02, Step: 2200/9567, Loss: 0.8658\n",
      "Epoch: 02, Step: 2300/9567, Loss: 0.8656\n",
      "Epoch: 02, Step: 2400/9567, Loss: 0.8609\n",
      "Epoch: 02, Step: 2500/9567, Loss: 0.8618\n",
      "Epoch: 02, Step: 2600/9567, Loss: 0.8642\n",
      "Epoch: 02, Step: 2700/9567, Loss: 0.8600\n",
      "Epoch: 02, Step: 2800/9567, Loss: 0.8659\n",
      "Epoch: 02, Step: 2900/9567, Loss: 0.8633\n",
      "Epoch: 02, Step: 3000/9567, Loss: 0.8618\n",
      "Epoch: 02, Step: 3100/9567, Loss: 0.8616\n",
      "Epoch: 02, Step: 3200/9567, Loss: 0.8571\n",
      "Epoch: 02, Step: 3300/9567, Loss: 0.8627\n",
      "Epoch: 02, Step: 3400/9567, Loss: 0.8632\n",
      "Epoch: 02, Step: 3500/9567, Loss: 0.8615\n",
      "Epoch: 02, Step: 3600/9567, Loss: 0.8620\n",
      "Epoch: 02, Step: 3700/9567, Loss: 0.8592\n",
      "Epoch: 02, Step: 3800/9567, Loss: 0.8632\n",
      "Epoch: 02, Step: 3900/9567, Loss: 0.8638\n",
      "Epoch: 02, Step: 4000/9567, Loss: 0.8617\n",
      "Epoch: 02, Step: 4100/9567, Loss: 0.8651\n",
      "Epoch: 02, Step: 4200/9567, Loss: 0.8585\n",
      "Epoch: 02, Step: 4300/9567, Loss: 0.8605\n",
      "Epoch: 02, Step: 4400/9567, Loss: 0.8619\n",
      "Epoch: 02, Step: 4500/9567, Loss: 0.8568\n",
      "Epoch: 02, Step: 4600/9567, Loss: 0.8569\n",
      "Epoch: 02, Step: 4700/9567, Loss: 0.8610\n",
      "Epoch: 02, Step: 4800/9567, Loss: 0.8539\n",
      "Epoch: 02, Step: 4900/9567, Loss: 0.8576\n",
      "Epoch: 02, Step: 5000/9567, Loss: 0.8626\n",
      "Epoch: 02, Step: 5100/9567, Loss: 0.8635\n",
      "Epoch: 02, Step: 5200/9567, Loss: 0.8593\n",
      "Epoch: 02, Step: 5300/9567, Loss: 0.8628\n",
      "Epoch: 02, Step: 5400/9567, Loss: 0.8618\n",
      "Epoch: 02, Step: 5500/9567, Loss: 0.8584\n",
      "Epoch: 02, Step: 5600/9567, Loss: 0.8560\n",
      "Epoch: 02, Step: 5700/9567, Loss: 0.8642\n",
      "Epoch: 02, Step: 5800/9567, Loss: 0.8608\n",
      "Epoch: 02, Step: 5900/9567, Loss: 0.8544\n",
      "Epoch: 02, Step: 6000/9567, Loss: 0.8581\n",
      "Epoch: 02, Step: 6100/9567, Loss: 0.8580\n",
      "Epoch: 02, Step: 6200/9567, Loss: 0.8588\n",
      "Epoch: 02, Step: 6300/9567, Loss: 0.8615\n",
      "Epoch: 02, Step: 6400/9567, Loss: 0.8598\n",
      "Epoch: 02, Step: 6500/9567, Loss: 0.8636\n",
      "Epoch: 02, Step: 6600/9567, Loss: 0.8572\n",
      "Epoch: 02, Step: 6700/9567, Loss: 0.8627\n",
      "Epoch: 02, Step: 6800/9567, Loss: 0.8611\n",
      "Epoch: 02, Step: 6900/9567, Loss: 0.8608\n",
      "Epoch: 02, Step: 7000/9567, Loss: 0.8503\n",
      "Epoch: 02, Step: 7100/9567, Loss: 0.8597\n",
      "Epoch: 02, Step: 7200/9567, Loss: 0.8620\n",
      "Epoch: 02, Step: 7300/9567, Loss: 0.8612\n",
      "Epoch: 02, Step: 7400/9567, Loss: 0.8569\n",
      "Epoch: 02, Step: 7500/9567, Loss: 0.8627\n",
      "Epoch: 02, Step: 7600/9567, Loss: 0.8597\n",
      "Epoch: 02, Step: 7700/9567, Loss: 0.8579\n",
      "Epoch: 02, Step: 7800/9567, Loss: 0.8627\n",
      "Epoch: 02, Step: 7900/9567, Loss: 0.8612\n",
      "Epoch: 02, Step: 8000/9567, Loss: 0.8579\n",
      "Epoch: 02, Step: 8100/9567, Loss: 0.8570\n",
      "Epoch: 02, Step: 8200/9567, Loss: 0.8549\n",
      "Epoch: 02, Step: 8300/9567, Loss: 0.8567\n",
      "Epoch: 02, Step: 8400/9567, Loss: 0.8596\n",
      "Epoch: 02, Step: 8500/9567, Loss: 0.8521\n",
      "Epoch: 02, Step: 8600/9567, Loss: 0.8585\n",
      "Epoch: 02, Step: 8700/9567, Loss: 0.8586\n",
      "Epoch: 02, Step: 8800/9567, Loss: 0.8587\n",
      "Epoch: 02, Step: 8900/9567, Loss: 0.8583\n",
      "Epoch: 02, Step: 9000/9567, Loss: 0.8609\n",
      "Epoch: 02, Step: 9100/9567, Loss: 0.8574\n",
      "Epoch: 02, Step: 9200/9567, Loss: 0.8533\n",
      "Epoch: 02, Step: 9300/9567, Loss: 0.8584\n",
      "Epoch: 02, Step: 9400/9567, Loss: 0.8560\n",
      "Epoch: 02, Step: 9500/9567, Loss: 0.8591\n",
      "Epoch: 03, Step: 100/9567, Loss: 0.8585\n",
      "Epoch: 03, Step: 200/9567, Loss: 0.8548\n",
      "Epoch: 03, Step: 300/9567, Loss: 0.8581\n",
      "Epoch: 03, Step: 400/9567, Loss: 0.8562\n",
      "Epoch: 03, Step: 500/9567, Loss: 0.8576\n",
      "Epoch: 03, Step: 600/9567, Loss: 0.8557\n",
      "Epoch: 03, Step: 700/9567, Loss: 0.8524\n",
      "Epoch: 03, Step: 800/9567, Loss: 0.8588\n",
      "Epoch: 03, Step: 900/9567, Loss: 0.8590\n",
      "Epoch: 03, Step: 1000/9567, Loss: 0.8535\n",
      "Epoch: 03, Step: 1100/9567, Loss: 0.8563\n",
      "Epoch: 03, Step: 1200/9567, Loss: 0.8532\n",
      "Epoch: 03, Step: 1300/9567, Loss: 0.8528\n",
      "Epoch: 03, Step: 1400/9567, Loss: 0.8575\n",
      "Epoch: 03, Step: 1500/9567, Loss: 0.8525\n",
      "Epoch: 03, Step: 1600/9567, Loss: 0.8568\n",
      "Epoch: 03, Step: 1700/9567, Loss: 0.8532\n",
      "Epoch: 03, Step: 1800/9567, Loss: 0.8579\n",
      "Epoch: 03, Step: 1900/9567, Loss: 0.8577\n",
      "Epoch: 03, Step: 2000/9567, Loss: 0.8532\n",
      "Epoch: 03, Step: 2100/9567, Loss: 0.8542\n",
      "Epoch: 03, Step: 2200/9567, Loss: 0.8569\n",
      "Epoch: 03, Step: 2300/9567, Loss: 0.8536\n",
      "Epoch: 03, Step: 2400/9567, Loss: 0.8514\n",
      "Epoch: 03, Step: 2500/9567, Loss: 0.8572\n",
      "Epoch: 03, Step: 2600/9567, Loss: 0.8560\n",
      "Epoch: 03, Step: 2700/9567, Loss: 0.8566\n",
      "Epoch: 03, Step: 2800/9567, Loss: 0.8543\n",
      "Epoch: 03, Step: 2900/9567, Loss: 0.8529\n",
      "Epoch: 03, Step: 3000/9567, Loss: 0.8551\n",
      "Epoch: 03, Step: 3100/9567, Loss: 0.8546\n",
      "Epoch: 03, Step: 3200/9567, Loss: 0.8518\n",
      "Epoch: 03, Step: 3300/9567, Loss: 0.8535\n",
      "Epoch: 03, Step: 3400/9567, Loss: 0.8499\n",
      "Epoch: 03, Step: 3500/9567, Loss: 0.8558\n",
      "Epoch: 03, Step: 3600/9567, Loss: 0.8563\n",
      "Epoch: 03, Step: 3700/9567, Loss: 0.8574\n",
      "Epoch: 03, Step: 3800/9567, Loss: 0.8592\n",
      "Epoch: 03, Step: 3900/9567, Loss: 0.8564\n",
      "Epoch: 03, Step: 4000/9567, Loss: 0.8557\n",
      "Epoch: 03, Step: 4100/9567, Loss: 0.8552\n",
      "Epoch: 03, Step: 4200/9567, Loss: 0.8496\n",
      "Epoch: 03, Step: 4300/9567, Loss: 0.8564\n",
      "Epoch: 03, Step: 4400/9567, Loss: 0.8488\n",
      "Epoch: 03, Step: 4500/9567, Loss: 0.8561\n",
      "Epoch: 03, Step: 4600/9567, Loss: 0.8508\n",
      "Epoch: 03, Step: 4700/9567, Loss: 0.8554\n",
      "Epoch: 03, Step: 4800/9567, Loss: 0.8499\n",
      "Epoch: 03, Step: 4900/9567, Loss: 0.8546\n",
      "Epoch: 03, Step: 5000/9567, Loss: 0.8529\n",
      "Epoch: 03, Step: 5100/9567, Loss: 0.8511\n",
      "Epoch: 03, Step: 5200/9567, Loss: 0.8588\n",
      "Epoch: 03, Step: 5300/9567, Loss: 0.8609\n",
      "Epoch: 03, Step: 5400/9567, Loss: 0.8579\n",
      "Epoch: 03, Step: 5500/9567, Loss: 0.8592\n",
      "Epoch: 03, Step: 5600/9567, Loss: 0.8548\n",
      "Epoch: 03, Step: 5700/9567, Loss: 0.8538\n",
      "Epoch: 03, Step: 5800/9567, Loss: 0.8586\n",
      "Epoch: 03, Step: 5900/9567, Loss: 0.8535\n",
      "Epoch: 03, Step: 6000/9567, Loss: 0.8510\n",
      "Epoch: 03, Step: 6100/9567, Loss: 0.8531\n",
      "Epoch: 03, Step: 6200/9567, Loss: 0.8505\n",
      "Epoch: 03, Step: 6300/9567, Loss: 0.8549\n",
      "Epoch: 03, Step: 6400/9567, Loss: 0.8528\n",
      "Epoch: 03, Step: 6500/9567, Loss: 0.8526\n",
      "Epoch: 03, Step: 6600/9567, Loss: 0.8517\n",
      "Epoch: 03, Step: 6700/9567, Loss: 0.8571\n",
      "Epoch: 03, Step: 6800/9567, Loss: 0.8587\n",
      "Epoch: 03, Step: 6900/9567, Loss: 0.8536\n",
      "Epoch: 03, Step: 7000/9567, Loss: 0.8544\n",
      "Epoch: 03, Step: 7100/9567, Loss: 0.8533\n",
      "Epoch: 03, Step: 7200/9567, Loss: 0.8569\n",
      "Epoch: 03, Step: 7300/9567, Loss: 0.8528\n",
      "Epoch: 03, Step: 7400/9567, Loss: 0.8579\n",
      "Epoch: 03, Step: 7500/9567, Loss: 0.8555\n",
      "Epoch: 03, Step: 7600/9567, Loss: 0.8458\n",
      "Epoch: 03, Step: 7700/9567, Loss: 0.8544\n",
      "Epoch: 03, Step: 7800/9567, Loss: 0.8529\n",
      "Epoch: 03, Step: 7900/9567, Loss: 0.8562\n",
      "Epoch: 03, Step: 8000/9567, Loss: 0.8544\n",
      "Epoch: 03, Step: 8100/9567, Loss: 0.8514\n",
      "Epoch: 03, Step: 8200/9567, Loss: 0.8523\n",
      "Epoch: 03, Step: 8300/9567, Loss: 0.8513\n",
      "Epoch: 03, Step: 8400/9567, Loss: 0.8537\n",
      "Epoch: 03, Step: 8500/9567, Loss: 0.8517\n",
      "Epoch: 03, Step: 8600/9567, Loss: 0.8578\n",
      "Epoch: 03, Step: 8700/9567, Loss: 0.8542\n",
      "Epoch: 03, Step: 8800/9567, Loss: 0.8521\n",
      "Epoch: 03, Step: 8900/9567, Loss: 0.8541\n",
      "Epoch: 03, Step: 9000/9567, Loss: 0.8551\n",
      "Epoch: 03, Step: 9100/9567, Loss: 0.8581\n",
      "Epoch: 03, Step: 9200/9567, Loss: 0.8529\n",
      "Epoch: 03, Step: 9300/9567, Loss: 0.8521\n",
      "Epoch: 03, Step: 9400/9567, Loss: 0.8544\n",
      "Epoch: 03, Step: 9500/9567, Loss: 0.8583\n",
      "Epoch: 04, Step: 100/9567, Loss: 0.8545\n",
      "Epoch: 04, Step: 200/9567, Loss: 0.8506\n",
      "Epoch: 04, Step: 300/9567, Loss: 0.8575\n",
      "Epoch: 04, Step: 400/9567, Loss: 0.8504\n",
      "Epoch: 04, Step: 500/9567, Loss: 0.8488\n",
      "Epoch: 04, Step: 600/9567, Loss: 0.8551\n",
      "Epoch: 04, Step: 700/9567, Loss: 0.8534\n",
      "Epoch: 04, Step: 800/9567, Loss: 0.8504\n",
      "Epoch: 04, Step: 900/9567, Loss: 0.8550\n",
      "Epoch: 04, Step: 1000/9567, Loss: 0.8473\n",
      "Epoch: 04, Step: 1100/9567, Loss: 0.8579\n",
      "Epoch: 04, Step: 1200/9567, Loss: 0.8530\n",
      "Epoch: 04, Step: 1300/9567, Loss: 0.8493\n",
      "Epoch: 04, Step: 1400/9567, Loss: 0.8550\n",
      "Epoch: 04, Step: 1500/9567, Loss: 0.8533\n",
      "Epoch: 04, Step: 1600/9567, Loss: 0.8605\n",
      "Epoch: 04, Step: 1700/9567, Loss: 0.8520\n",
      "Epoch: 04, Step: 1800/9567, Loss: 0.8527\n",
      "Epoch: 04, Step: 1900/9567, Loss: 0.8526\n",
      "Epoch: 04, Step: 2000/9567, Loss: 0.8523\n",
      "Epoch: 04, Step: 2100/9567, Loss: 0.8530\n",
      "Epoch: 04, Step: 2200/9567, Loss: 0.8545\n",
      "Epoch: 04, Step: 2300/9567, Loss: 0.8542\n",
      "Epoch: 04, Step: 2400/9567, Loss: 0.8488\n",
      "Epoch: 04, Step: 2500/9567, Loss: 0.8544\n",
      "Epoch: 04, Step: 2600/9567, Loss: 0.8554\n",
      "Epoch: 04, Step: 2700/9567, Loss: 0.8507\n",
      "Epoch: 04, Step: 2800/9567, Loss: 0.8561\n",
      "Epoch: 04, Step: 2900/9567, Loss: 0.8531\n",
      "Epoch: 04, Step: 3000/9567, Loss: 0.8545\n",
      "Epoch: 04, Step: 3100/9567, Loss: 0.8592\n",
      "Epoch: 04, Step: 3200/9567, Loss: 0.8565\n",
      "Epoch: 04, Step: 3300/9567, Loss: 0.8482\n",
      "Epoch: 04, Step: 3400/9567, Loss: 0.8544\n",
      "Epoch: 04, Step: 3500/9567, Loss: 0.8543\n",
      "Epoch: 04, Step: 3600/9567, Loss: 0.8607\n",
      "Epoch: 04, Step: 3700/9567, Loss: 0.8525\n",
      "Epoch: 04, Step: 3800/9567, Loss: 0.8542\n",
      "Epoch: 04, Step: 3900/9567, Loss: 0.8562\n",
      "Epoch: 04, Step: 4000/9567, Loss: 0.8537\n",
      "Epoch: 04, Step: 4100/9567, Loss: 0.8551\n",
      "Epoch: 04, Step: 4200/9567, Loss: 0.8478\n",
      "Epoch: 04, Step: 4300/9567, Loss: 0.8530\n",
      "Epoch: 04, Step: 4400/9567, Loss: 0.8525\n",
      "Epoch: 04, Step: 4500/9567, Loss: 0.8458\n",
      "Epoch: 04, Step: 4600/9567, Loss: 0.8550\n",
      "Epoch: 04, Step: 4700/9567, Loss: 0.8547\n",
      "Epoch: 04, Step: 4800/9567, Loss: 0.8531\n",
      "Epoch: 04, Step: 4900/9567, Loss: 0.8502\n",
      "Epoch: 04, Step: 5000/9567, Loss: 0.8588\n",
      "Epoch: 04, Step: 5100/9567, Loss: 0.8534\n",
      "Epoch: 04, Step: 5200/9567, Loss: 0.8514\n",
      "Epoch: 04, Step: 5300/9567, Loss: 0.8529\n",
      "Epoch: 04, Step: 5400/9567, Loss: 0.8530\n",
      "Epoch: 04, Step: 5500/9567, Loss: 0.8522\n",
      "Epoch: 04, Step: 5600/9567, Loss: 0.8586\n",
      "Epoch: 04, Step: 5700/9567, Loss: 0.8539\n",
      "Epoch: 04, Step: 5800/9567, Loss: 0.8546\n",
      "Epoch: 04, Step: 5900/9567, Loss: 0.8533\n",
      "Epoch: 04, Step: 6000/9567, Loss: 0.8574\n",
      "Epoch: 04, Step: 6100/9567, Loss: 0.8580\n",
      "Epoch: 04, Step: 6200/9567, Loss: 0.8500\n",
      "Epoch: 04, Step: 6300/9567, Loss: 0.8538\n",
      "Epoch: 04, Step: 6400/9567, Loss: 0.8542\n",
      "Epoch: 04, Step: 6500/9567, Loss: 0.8576\n",
      "Epoch: 04, Step: 6600/9567, Loss: 0.8570\n",
      "Epoch: 04, Step: 6700/9567, Loss: 0.8532\n",
      "Epoch: 04, Step: 6800/9567, Loss: 0.8554\n",
      "Epoch: 04, Step: 6900/9567, Loss: 0.8539\n",
      "Epoch: 04, Step: 7000/9567, Loss: 0.8563\n",
      "Epoch: 04, Step: 7100/9567, Loss: 0.8544\n",
      "Epoch: 04, Step: 7200/9567, Loss: 0.8576\n",
      "Epoch: 04, Step: 7300/9567, Loss: 0.8595\n",
      "Epoch: 04, Step: 7400/9567, Loss: 0.8511\n",
      "Epoch: 04, Step: 7500/9567, Loss: 0.8514\n",
      "Epoch: 04, Step: 7600/9567, Loss: 0.8496\n",
      "Epoch: 04, Step: 7700/9567, Loss: 0.8517\n",
      "Epoch: 04, Step: 7800/9567, Loss: 0.8520\n",
      "Epoch: 04, Step: 7900/9567, Loss: 0.8532\n",
      "Epoch: 04, Step: 8000/9567, Loss: 0.8543\n",
      "Epoch: 04, Step: 8100/9567, Loss: 0.8532\n",
      "Epoch: 04, Step: 8200/9567, Loss: 0.8529\n",
      "Epoch: 04, Step: 8300/9567, Loss: 0.8513\n",
      "Epoch: 04, Step: 8400/9567, Loss: 0.8542\n",
      "Epoch: 04, Step: 8500/9567, Loss: 0.8491\n",
      "Epoch: 04, Step: 8600/9567, Loss: 0.8578\n",
      "Epoch: 04, Step: 8700/9567, Loss: 0.8523\n",
      "Epoch: 04, Step: 8800/9567, Loss: 0.8592\n",
      "Epoch: 04, Step: 8900/9567, Loss: 0.8558\n",
      "Epoch: 04, Step: 9000/9567, Loss: 0.8528\n",
      "Epoch: 04, Step: 9100/9567, Loss: 0.8534\n",
      "Epoch: 04, Step: 9200/9567, Loss: 0.8553\n",
      "Epoch: 04, Step: 9300/9567, Loss: 0.8596\n",
      "Epoch: 04, Step: 9400/9567, Loss: 0.8491\n",
      "Epoch: 04, Step: 9500/9567, Loss: 0.8566\n",
      "Epoch: 05, Step: 100/9567, Loss: 0.8539\n",
      "Epoch: 05, Step: 200/9567, Loss: 0.8523\n",
      "Epoch: 05, Step: 300/9567, Loss: 0.8563\n",
      "Epoch: 05, Step: 400/9567, Loss: 0.8504\n",
      "Epoch: 05, Step: 500/9567, Loss: 0.8534\n",
      "Epoch: 05, Step: 600/9567, Loss: 0.8479\n",
      "Epoch: 05, Step: 700/9567, Loss: 0.8522\n",
      "Epoch: 05, Step: 800/9567, Loss: 0.8536\n",
      "Epoch: 05, Step: 900/9567, Loss: 0.8493\n",
      "Epoch: 05, Step: 1000/9567, Loss: 0.8510\n",
      "Epoch: 05, Step: 1100/9567, Loss: 0.8495\n",
      "Epoch: 05, Step: 1200/9567, Loss: 0.8516\n",
      "Epoch: 05, Step: 1300/9567, Loss: 0.8554\n",
      "Epoch: 05, Step: 1400/9567, Loss: 0.8444\n",
      "Epoch: 05, Step: 1500/9567, Loss: 0.8524\n",
      "Epoch: 05, Step: 1600/9567, Loss: 0.8490\n",
      "Epoch: 05, Step: 1700/9567, Loss: 0.8543\n",
      "Epoch: 05, Step: 1800/9567, Loss: 0.8485\n",
      "Epoch: 05, Step: 1900/9567, Loss: 0.8502\n",
      "Epoch: 05, Step: 2000/9567, Loss: 0.8524\n",
      "Epoch: 05, Step: 2100/9567, Loss: 0.8509\n",
      "Epoch: 05, Step: 2200/9567, Loss: 0.8515\n",
      "Epoch: 05, Step: 2300/9567, Loss: 0.8490\n",
      "Epoch: 05, Step: 2400/9567, Loss: 0.8527\n",
      "Epoch: 05, Step: 2500/9567, Loss: 0.8504\n",
      "Epoch: 05, Step: 2600/9567, Loss: 0.8538\n",
      "Epoch: 05, Step: 2700/9567, Loss: 0.8482\n",
      "Epoch: 05, Step: 2800/9567, Loss: 0.8494\n",
      "Epoch: 05, Step: 2900/9567, Loss: 0.8516\n",
      "Epoch: 05, Step: 3000/9567, Loss: 0.8506\n",
      "Epoch: 05, Step: 3100/9567, Loss: 0.8546\n",
      "Epoch: 05, Step: 3200/9567, Loss: 0.8461\n",
      "Epoch: 05, Step: 3300/9567, Loss: 0.8545\n",
      "Epoch: 05, Step: 3400/9567, Loss: 0.8548\n",
      "Epoch: 05, Step: 3500/9567, Loss: 0.8491\n",
      "Epoch: 05, Step: 3600/9567, Loss: 0.8587\n",
      "Epoch: 05, Step: 3700/9567, Loss: 0.8508\n",
      "Epoch: 05, Step: 3800/9567, Loss: 0.8544\n",
      "Epoch: 05, Step: 3900/9567, Loss: 0.8494\n",
      "Epoch: 05, Step: 4000/9567, Loss: 0.8492\n",
      "Epoch: 05, Step: 4100/9567, Loss: 0.8528\n",
      "Epoch: 05, Step: 4200/9567, Loss: 0.8521\n",
      "Epoch: 05, Step: 4300/9567, Loss: 0.8515\n",
      "Epoch: 05, Step: 4400/9567, Loss: 0.8509\n",
      "Epoch: 05, Step: 4500/9567, Loss: 0.8468\n",
      "Epoch: 05, Step: 4600/9567, Loss: 0.8533\n",
      "Epoch: 05, Step: 4700/9567, Loss: 0.8528\n",
      "Epoch: 05, Step: 4800/9567, Loss: 0.8501\n",
      "Epoch: 05, Step: 4900/9567, Loss: 0.8524\n",
      "Epoch: 05, Step: 5000/9567, Loss: 0.8494\n",
      "Epoch: 05, Step: 5100/9567, Loss: 0.8489\n",
      "Epoch: 05, Step: 5200/9567, Loss: 0.8538\n",
      "Epoch: 05, Step: 5300/9567, Loss: 0.8537\n",
      "Epoch: 05, Step: 5400/9567, Loss: 0.8500\n",
      "Epoch: 05, Step: 5500/9567, Loss: 0.8503\n",
      "Epoch: 05, Step: 5600/9567, Loss: 0.8555\n",
      "Epoch: 05, Step: 5700/9567, Loss: 0.8485\n",
      "Epoch: 05, Step: 5800/9567, Loss: 0.8587\n",
      "Epoch: 05, Step: 5900/9567, Loss: 0.8540\n",
      "Epoch: 05, Step: 6000/9567, Loss: 0.8528\n",
      "Epoch: 05, Step: 6100/9567, Loss: 0.8503\n",
      "Epoch: 05, Step: 6200/9567, Loss: 0.8500\n",
      "Epoch: 05, Step: 6300/9567, Loss: 0.8525\n",
      "Epoch: 05, Step: 6400/9567, Loss: 0.8533\n",
      "Epoch: 05, Step: 6500/9567, Loss: 0.8516\n",
      "Epoch: 05, Step: 6600/9567, Loss: 0.8499\n",
      "Epoch: 05, Step: 6700/9567, Loss: 0.8501\n",
      "Epoch: 05, Step: 6800/9567, Loss: 0.8552\n",
      "Epoch: 05, Step: 6900/9567, Loss: 0.8535\n",
      "Epoch: 05, Step: 7000/9567, Loss: 0.8541\n",
      "Epoch: 05, Step: 7100/9567, Loss: 0.8562\n",
      "Epoch: 05, Step: 7200/9567, Loss: 0.8528\n",
      "Epoch: 05, Step: 7300/9567, Loss: 0.8529\n",
      "Epoch: 05, Step: 7400/9567, Loss: 0.8570\n",
      "Epoch: 05, Step: 7500/9567, Loss: 0.8493\n",
      "Epoch: 05, Step: 7600/9567, Loss: 0.8528\n",
      "Epoch: 05, Step: 7700/9567, Loss: 0.8545\n",
      "Epoch: 05, Step: 7800/9567, Loss: 0.8499\n",
      "Epoch: 05, Step: 7900/9567, Loss: 0.8479\n",
      "Epoch: 05, Step: 8000/9567, Loss: 0.8513\n",
      "Epoch: 05, Step: 8100/9567, Loss: 0.8528\n",
      "Epoch: 05, Step: 8200/9567, Loss: 0.8516\n",
      "Epoch: 05, Step: 8300/9567, Loss: 0.8543\n",
      "Epoch: 05, Step: 8400/9567, Loss: 0.8517\n",
      "Epoch: 05, Step: 8500/9567, Loss: 0.8501\n",
      "Epoch: 05, Step: 8600/9567, Loss: 0.8540\n",
      "Epoch: 05, Step: 8700/9567, Loss: 0.8498\n",
      "Epoch: 05, Step: 8800/9567, Loss: 0.8516\n",
      "Epoch: 05, Step: 8900/9567, Loss: 0.8543\n",
      "Epoch: 05, Step: 9000/9567, Loss: 0.8541\n",
      "Epoch: 05, Step: 9100/9567, Loss: 0.8529\n",
      "Epoch: 05, Step: 9200/9567, Loss: 0.8553\n",
      "Epoch: 05, Step: 9300/9567, Loss: 0.8523\n",
      "Epoch: 05, Step: 9400/9567, Loss: 0.8539\n",
      "Epoch: 05, Step: 9500/9567, Loss: 0.8534\n",
      "Epoch: 06, Step: 100/9567, Loss: 0.8518\n",
      "Epoch: 06, Step: 200/9567, Loss: 0.8523\n",
      "Epoch: 06, Step: 300/9567, Loss: 0.8531\n",
      "Epoch: 06, Step: 400/9567, Loss: 0.8529\n",
      "Epoch: 06, Step: 500/9567, Loss: 0.8485\n",
      "Epoch: 06, Step: 600/9567, Loss: 0.8511\n",
      "Epoch: 06, Step: 700/9567, Loss: 0.8520\n",
      "Epoch: 06, Step: 800/9567, Loss: 0.8518\n",
      "Epoch: 06, Step: 900/9567, Loss: 0.8469\n",
      "Epoch: 06, Step: 1000/9567, Loss: 0.8511\n",
      "Epoch: 06, Step: 1100/9567, Loss: 0.8497\n",
      "Epoch: 06, Step: 1200/9567, Loss: 0.8540\n",
      "Epoch: 06, Step: 1300/9567, Loss: 0.8512\n",
      "Epoch: 06, Step: 1400/9567, Loss: 0.8533\n",
      "Epoch: 06, Step: 1500/9567, Loss: 0.8526\n",
      "Epoch: 06, Step: 1600/9567, Loss: 0.8541\n",
      "Epoch: 06, Step: 1700/9567, Loss: 0.8498\n",
      "Epoch: 06, Step: 1800/9567, Loss: 0.8478\n",
      "Epoch: 06, Step: 1900/9567, Loss: 0.8532\n",
      "Epoch: 06, Step: 2000/9567, Loss: 0.8524\n",
      "Epoch: 06, Step: 2100/9567, Loss: 0.8545\n",
      "Epoch: 06, Step: 2200/9567, Loss: 0.8503\n",
      "Epoch: 06, Step: 2300/9567, Loss: 0.8450\n",
      "Epoch: 06, Step: 2400/9567, Loss: 0.8525\n",
      "Epoch: 06, Step: 2500/9567, Loss: 0.8537\n",
      "Epoch: 06, Step: 2600/9567, Loss: 0.8552\n",
      "Epoch: 06, Step: 2700/9567, Loss: 0.8432\n",
      "Epoch: 06, Step: 2800/9567, Loss: 0.8487\n",
      "Epoch: 06, Step: 2900/9567, Loss: 0.8546\n",
      "Epoch: 06, Step: 3000/9567, Loss: 0.8494\n",
      "Epoch: 06, Step: 3100/9567, Loss: 0.8522\n",
      "Epoch: 06, Step: 3200/9567, Loss: 0.8552\n",
      "Epoch: 06, Step: 3300/9567, Loss: 0.8543\n",
      "Epoch: 06, Step: 3400/9567, Loss: 0.8561\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-97d2d6a46a5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_rw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_rw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/latest_PyG/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/latest_PyG/lib/python3.7/site-packages/torch/optim/sparse_adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# the update is non-linear so indices must be unique\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0mgrad_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mgrad_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for i, (pos_rw, neg_rw) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % log_steps == 0:\n",
    "            print(f'Epoch: {epoch:02d}, Step: {i+1:03d}/{len(loader)}, '\n",
    "                  f'Loss: {loss:.4f}')\n",
    "\n",
    "        if (i + 1) % 100 == 0:  # Save model every 100 steps.\n",
    "            save_embedding(model)\n",
    "    save_embedding(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latest_PyG",
   "language": "python",
   "name": "latest_pyg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
